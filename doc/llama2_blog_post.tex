\documentclass[letterpaper,12pt]{article}
\usepackage{times}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{courier}
\usepackage{fancyheadings}
\usepackage{hyperref}
\usepackage{xcolor}
\pagestyle{fancy}
\usepackage{graphicx}
\usepackage{verbatim}
\setlength\textwidth{6.5in}
\setlength\textheight{8.5in}
\newcommand{\ISPC}{{\tt ispc}}
\newcommand{\TBC}{\framebox{\textbf{TO BE COMPLETED}}}
\newtheorem{assumption}{Assumption}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\bd}{\begin{description}}
\newcommand{\ed}{\end{description}}
\newtheorem{notation}{Notation}
\begin{document}
\title{The Case for Mechanical Empathy}
\author{Ramesh Subramonian }
\maketitle
\thispagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
% \lfoot{{\small Decision Sciences Team}}
\cfoot{}
\rfoot{{\small \thepage}}

\begin{abstract}

  The article documents the author's experience playing with Andrei
  Karpathy's excellent one file implementation of Llama-2. The TL;DR
  is that there is an interesting spot on the performance-programming
  effort continuum achieved by collaborating with the compiler. In
  particular, porting selected portions of Karpathy's code to \ISPC\
  yields a 2X performance gain.


\end{abstract}

\section{Introduction}

Karpathy's llama2.c project ``inferences Llama-2 with one simple
700-line C file (run.c).''  The code is well-written --- as the author
points out, it has been written with exposition and ease of
understanding in mind, not necessarily performance.

Nevertheless, we took this as a starting point to see how we could improve
performance without sacrificing readability. For this purpose, 
we ported selected portion of the code using \ISPC\ --- IntelÂ®
Implicit SPMD Program Compiler. For those unfamiliar with this excellent 
project, ``\ISPC\ is
a compiler for a variant of the C programming language, with extensions for
``single program, multiple data'' (SPMD) programming. ''
The documentation and tutorials provided by the ISPC authors is excellent. I
will not repeat that here, referring the user to \url{https://ispc.github.io}
instead. 

\section{Modifications to Karpathy's code}
\subsection{Cosmetic changes}
The cosmetic changes fall into the following categories
\be
\item We broke out several of the compute intensive functions
  into separate files. Localizing changes helped 
  tune and test smaller pieces without destabilizing the code.
\item Indentation styles can lead to  religious wars. I chose to use my own
  style
\item The command line parameter {\tt -q}  was added to indicate whether 
  weights had been quantized.
  \ee

\subsection{Substantive changes}
The most substantive change is covered in Section~\ref{ISPC_ports}

\subsubsection{Hint, hint, \ldots}

We provided as much guidance to the compiler as possible. For example,
\be
\item we used the {\tt restrict} type qualifier wherever possible. ``By adding
  this type qualifier, a programmer hints to the compiler that for the lifetime
  of the pointer, no other pointer will be used to access the object to which it
  points. This allows the compiler to make optimizations (for example,
  vectorization) that would not otherwise have been
  possible.''\footnote{\url{https://en.wikipedia.org/wiki/Restrict}}
\item we used the \verb+__attribute__(())+ syntax in \ISPC\ for variable and function
  declarations, thereby eliminating extra code and letting the compiler know
  about memory alignment
\item we used the {\tt assume()} mechanism in \ISPC\ where possible e.g., letting the
  compiler know that we have padded vectors so that they are multiples of 
  the
  lane width of the vector register.
  \ee

\subsubsection{Aligning memory}

To support vector instructions, we aligned memory on 32-byte boundaries.
\be
\item Karpathy {\tt mmap}s a single binary weights file for all the different
  kinds of weights. In contrast, we broke the single weights file into twelve separate files, 
one for each of the weight classes, padding the vectors to be multiples of the
vector register width. For example, a 2D array with 2 rows and 3 columns such as 
\(
\begin{bmatrix}
  1 & 2 & 3 \\ 
  10 & 20 & 30 \\ 
\end{bmatrix}
\)
is laid out in a linear array as shown below when padded to multiples of 4 
\begin{center}
\(\{1, 2, 3, 0, 10, 20, 30, 0\}\)
\end{center}
\item We used {\tt posix\_memalign()} instead of
{\tt malloc()} to ensure alignment. 
\item We padded all vectors, increasing memory usage somewhat, but ensuring that
  all vectors are properly aligned.
\ee


\subsubsection{Use of OpenMP}

\subsubsection{Quantization}

Quantization reduces memory pressure. In particular, 
we chose to quantize {\tt fp32} as {\tt uint8\_t}. In doing so,
each vector within a tensor was scaled independently. This means that a vector
of floating point numbers of the form 
\begin{center}
  \{  \(  f_1, f_2, \ldots f_N \}\)
\end{center}
would be represented as unsigned 8-bit integers with two additional pieces of
information 
\begin{center}
  \(\mathrm{min}(x), \mathrm{max}(x), \{u_1, u_2, \ldots u_N\} \)
\end{center}


\subsection{Nitpicks}

\be
\item Karpathy claims that 
  `` we use {\tt calloc()} instead of {\tt malloc()} to keep valgrind happy''. I found no evidence to suport that claim.
\ee


\subsection{Quantization}

Karpathy chose to support quantization by writing
a separate {\tt runq.c} file. We felt that there was sufficient commonality
between the original and the quantized version that  a separate implementation
was not justified. Our approach was 
\be
\item write a pre-processing utility to create quantized weights and scales.
\item memory map the quantized weights. For debugging purposes (not for
  production), the original weights are also memory mapped so that we can assess
  the lossiness of the compression
\item restrict the quantization to the larger weight classes, not the smaller
  ones 
\item Since the {\tt matmul()} function consumes most of the time, 
  restrict the changes to that function.
  In particular, the only change made is to the dot product function called from
  {\tt matmul()} where the lines shown in red are inserted. Note that the
  signature of the calling function also needed to change so that it had access
  to the scaling values of {\tt offset} and {\tt delta}.

  \begin{tabbing}\hspace*{0.25in} \= \kill
    {\bf export void} dot\_prod\_qnt( \+  \\
    \textcolor{red}{uniform float offset, uniform float delta,} \\
    uniform float x[], uniform uint8 y[], uniform int n, uniform float rslt[] \-
    \\
    ) \\

    {\bf foreach}  ( i = 0 \ldots n ) \{ \+ \\
     \textcolor{red}{{\bf varying float} y\_i = (y[i] * delta) + offset;} \\
    sum += x[i] * y\_i; \- \\ 
  \} \\
    rslt[0] = sum
  \end{tabbing}

\item Instead of using integer arithmetic on quantized values, we used floating
  point arithmetic after converting {\tt unit32\_t} back to {\tt float} on the
  fly.
  \ee

\section{ISPC Ports}
\label{ISPC_ports}

\section{Performance Results}

The hardware used for benchmarking was inspired by DHH's blog post
\url{https://world.hey.com/dhh/it-s-a-beelink-baby-243fdaf1}. It was a 
Beelink SEi14 Mini PC, powered by Intel Ultra 9 185H (up to 5.1GHz) 16C/22T, Mini Computer
with 64GB DDR5 at 5600MHz.

We used the same seed and the same prompt on the {\tt stories42M.bin} 
data set to generate a mini-story. Results are reported by
averaging over  20 runs after discarding the top 2 and the bottom 2 results.

We used {\tt gcc} for both versions, with flags including {\tt -O3 -Ofast -flto}.
For \ISPC, compiler options included
\begin{verbatim}
--addressing=32 --opt=fast-math 
--math-lib=fast --opt=force-aligned-memory
\end{verbatim}

Using Karpathy's code, it runs at XXX tokens/second. 
Our code, using ISPC, runs at YYY tokens/second. 


\section{Conclusion}


\subsection{Acknowledgements}

Thanks to Moh Haghighat for pointing me to ISPC in the first place. 
Many thanks to two excellent managers, Luis Stevens and Subbu Iyer, who
encouraged my dabbling with ISPC. 

\end{document}
